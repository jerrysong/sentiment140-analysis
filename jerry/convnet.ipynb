{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Characters level ConvNet paper: https://papers.nips.cc/paper/5782-character-level-convolutional-networks-for-text-classification.pdf\n",
    "# Reference: https://github.com/mhjabreel/CharCnn_Keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Concatenate, ThresholdedReLU, MaxPooling1D, Flatten, Dropout, ReLU, Activation\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.utils import multi_gpu_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/sentiment140_train.zip', encoding='ISO-8859-1', header=None, names=['sentiment','id','timestamp','type','user','text'])\n",
    "test_data = pd.read_csv('../data/sentiment140_test.zip', encoding='ISO-8859-1', header=None, names=['sentiment','id','timestamp','type','user','text'])\n",
    "\n",
    "# The original training data are sorted by sentiment value. Shuffle the training data for randomness\n",
    "train_data = train_data[['text','sentiment']].sample(frac=1, random_state=40)\n",
    "test_data = test_data[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[test_data['sentiment'].isin([0, 4])]\n",
    "test_data['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 41,\n",
       " '\"': 45,\n",
       " '#': 51,\n",
       " '$': 52,\n",
       " '%': 53,\n",
       " '&': 55,\n",
       " \"'\": 44,\n",
       " '(': 64,\n",
       " ')': 65,\n",
       " '*': 56,\n",
       " '+': 59,\n",
       " ',': 38,\n",
       " '-': 60,\n",
       " '.': 40,\n",
       " '/': 46,\n",
       " '0': 27,\n",
       " '1': 28,\n",
       " '2': 29,\n",
       " '3': 30,\n",
       " '4': 31,\n",
       " '5': 32,\n",
       " '6': 33,\n",
       " '7': 34,\n",
       " '8': 35,\n",
       " '9': 36,\n",
       " ':': 43,\n",
       " ';': 39,\n",
       " '<': 62,\n",
       " '=': 61,\n",
       " '>': 63,\n",
       " '?': 42,\n",
       " '@': 50,\n",
       " '[': 66,\n",
       " '\\\\': 47,\n",
       " ']': 67,\n",
       " '^': 54,\n",
       " '_': 49,\n",
       " '`': 58,\n",
       " 'a': 1,\n",
       " 'b': 2,\n",
       " 'c': 3,\n",
       " 'd': 4,\n",
       " 'e': 5,\n",
       " 'f': 6,\n",
       " 'g': 7,\n",
       " 'h': 8,\n",
       " 'i': 9,\n",
       " 'j': 10,\n",
       " 'k': 11,\n",
       " 'l': 12,\n",
       " 'm': 13,\n",
       " 'n': 14,\n",
       " 'o': 15,\n",
       " 'p': 16,\n",
       " 'q': 17,\n",
       " 'r': 18,\n",
       " 's': 19,\n",
       " 't': 20,\n",
       " 'u': 21,\n",
       " 'v': 22,\n",
       " 'w': 23,\n",
       " 'x': 24,\n",
       " 'y': 25,\n",
       " 'z': 26,\n",
       " '{': 68,\n",
       " '|': 48,\n",
       " '}': 69,\n",
       " '~': 57}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz0123456789-,;.!?:\\'\"/\\\\|_@#$%^&*~`+-=<>()[]{}'\n",
    "alphabet_size = len(alphabet)\n",
    "alphabet_index = {alphabet[i]: i + 1 for i in range(alphabet_size)}\n",
    "alphabet_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_size = 374"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 100)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = []\n",
    "for _, row in train_data.iterrows():\n",
    "    str2idx = np.zeros(max_input_size, dtype='int64')\n",
    "    for i, letter in enumerate(row['text'].lower()):\n",
    "        if i == max_input_size:\n",
    "            break\n",
    "        str2idx[i] = alphabet_index.get(letter, 0)\n",
    "    X_train.append(str2idx)\n",
    "    \n",
    "X_train = np.array(X_train)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(359, 100)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test = []\n",
    "for _, row in test_data.iterrows():\n",
    "    str2idx = np.zeros(max_input_size, dtype='int64')\n",
    "    for i, letter in enumerate(row['text'].lower()):\n",
    "        if i == max_input_size:\n",
    "            break\n",
    "        str2idx[i] = alphabet_index.get(letter, 0)\n",
    "    X_test.append(str2idx)\n",
    "    \n",
    "X_test = np.array(X_test)\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = pd.get_dummies(train_data['sentiment']).values\n",
    "Y_test = pd.get_dummies(test_data['sentiment']).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('x_train', X_train)\n",
    "np.save('x_test', X_test)\n",
    "np.save('y_train', Y_train)\n",
    "np.save('y_test', Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_size = 374\n",
    "X_train = np.load('x_train.npy')\n",
    "X_test = np.load('x_test.npy')\n",
    "Y_train = np.load('y_train.npy')\n",
    "Y_test = np.load('y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600000, 374) (1600000, 2)\n",
      "(359, 374) (359, 2)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, Y_train.shape)\n",
    "print(X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNNZhang(object):\n",
    "    \"\"\"\n",
    "    Class to implement the Character Level Convolutional Neural Network for Text Classification,\n",
    "    as described in Zhang et al., 2015 (http://arxiv.org/abs/1509.01626)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, alphabet_size, embedding_size,\n",
    "                 conv_layers, fully_connected_layers, num_of_classes,\n",
    "                 threshold, dropout_p, learning_rate, loss='categorical_crossentropy'):\n",
    "        \"\"\"\n",
    "        Initialization for the Character Level CNN model.\n",
    "        Args:\n",
    "            input_size (int): Size of input features\n",
    "            alphabet_size (int): Size of alphabets to create embeddings for\n",
    "            embedding_size (int): Size of embeddings\n",
    "            conv_layers (list[list[int]]): List of Convolution layers for model\n",
    "            fully_connected_layers (list[list[int]]): List of Fully Connected layers for model\n",
    "            num_of_classes (int): Number of classes in data\n",
    "            threshold (float): Threshold for Thresholded ReLU activation function\n",
    "            dropout_p (float): Dropout Probability\n",
    "            optimizer (str): Training optimizer\n",
    "            loss (str): Loss function\n",
    "        \"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.alphabet_size = alphabet_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.conv_layers = conv_layers\n",
    "        self.fully_connected_layers = fully_connected_layers\n",
    "        self.num_of_classes = num_of_classes\n",
    "        self.threshold = threshold\n",
    "        self.dropout_p = dropout_p\n",
    "        self.optimizer = Adam(learning_rate)\n",
    "        self.loss = loss\n",
    "        self._build_model()  # builds self.model variable\n",
    "\n",
    "    def _build_model(self):\n",
    "        \"\"\"\n",
    "        Build and compile the Character Level CNN model\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        # Input layer\n",
    "        inputs = Input(shape=(self.input_size,), name='sent_input')\n",
    "        # Embedding layers\n",
    "        x = Embedding(self.alphabet_size + 1, self.embedding_size, input_length=self.input_size)(inputs)\n",
    "        # Convolution layers\n",
    "        for cl in self.conv_layers:\n",
    "            x = Convolution1D(cl[0], cl[1])(x)\n",
    "            x = ThresholdedReLU(self.threshold)(x)\n",
    "            if cl[2] != -1:\n",
    "                x = MaxPooling1D(cl[2])(x)\n",
    "        x = Flatten()(x)\n",
    "        # Fully connected layers\n",
    "        for fl in self.fully_connected_layers:\n",
    "            x = Dense(fl)(x)\n",
    "            x = ThresholdedReLU(self.threshold)(x)\n",
    "            x = Dropout(self.dropout_p)(x)\n",
    "        # Output layer\n",
    "        predictions = Dense(self.num_of_classes, activation='softmax')(x)\n",
    "        # Build and compile model\n",
    "        model = Model(inputs=inputs, outputs=predictions)\n",
    "        # model = multi_gpu_model(model, 2, cpu_relocation=True)\n",
    "        model.compile(optimizer=self.optimizer, loss=self.loss, metrics=['accuracy'])\n",
    "        self.model = model\n",
    "        print(\"CharCNNZhang model built: \")\n",
    "        self.model.summary()\n",
    "\n",
    "\n",
    "    def train(self, training_inputs, training_labels, validation_split,\n",
    "              epochs, batch_size, checkpoint_every=100):\n",
    "        \"\"\"\n",
    "        Training function\n",
    "        Args:\n",
    "            training_inputs (numpy.ndarray): Training set inputs\n",
    "            training_labels (numpy.ndarray): Training set labels\n",
    "            epochs (int): Number of training epochs\n",
    "            batch_size (int): Batch size\n",
    "            checkpoint_every (int): Interval for logging to Tensorboard\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        # Create callbacks\n",
    "        tensorboard = TensorBoard(log_dir='./logs', histogram_freq=checkpoint_every, batch_size=batch_size,\n",
    "                                  write_graph=False, write_grads=True, write_images=False,\n",
    "                                  embeddings_freq=checkpoint_every,\n",
    "                                  embeddings_layer_names=None)\n",
    "        # Start training\n",
    "        print(\"Training CharCNNZhang model: \")\n",
    "        return self.model.fit(training_inputs, training_labels,\n",
    "                       validation_split=validation_split,\n",
    "                       epochs=epochs,\n",
    "                       batch_size=batch_size,\n",
    "                       verbose=2)\n",
    "\n",
    "    def test(self, testing_inputs, testing_labels, batch_size):\n",
    "        \"\"\"\n",
    "        Testing function\n",
    "        Args:\n",
    "            testing_inputs (numpy.ndarray): Testing set inputs\n",
    "            testing_labels (numpy.ndarray): Testing set labels\n",
    "            batch_size (int): Batch size\n",
    "        Returns: None\n",
    "        \"\"\"\n",
    "        # Evaluate inputs\n",
    "        return self.model.evaluate(testing_inputs, testing_labels, batch_size=batch_size, verbose=2)\n",
    "        # self.model.predict(testing_inputs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharCNNZhang model built: \n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sent_input (InputLayer)      (None, 374)               0         \n",
      "_________________________________________________________________\n",
      "embedding_33 (Embedding)     (None, 374, 128)          8960      \n",
      "_________________________________________________________________\n",
      "conv1d_181 (Conv1D)          (None, 368, 256)          229632    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_201 (Thres (None, 368, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_88 (MaxPooling (None, 122, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_182 (Conv1D)          (None, 116, 256)          459008    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_202 (Thres (None, 116, 256)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_89 (MaxPooling (None, 38, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_183 (Conv1D)          (None, 36, 256)           196864    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_203 (Thres (None, 36, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_184 (Conv1D)          (None, 34, 256)           196864    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_204 (Thres (None, 34, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_185 (Conv1D)          (None, 32, 256)           196864    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_205 (Thres (None, 32, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_186 (Conv1D)          (None, 30, 256)           196864    \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_206 (Thres (None, 30, 256)           0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_90 (MaxPooling (None, 10, 256)           0         \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 2560)              0         \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 1024)              2622464   \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_207 (Thres (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 1024)              1049600   \n",
      "_________________________________________________________________\n",
      "thresholded_re_lu_208 (Thres (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dropout_25 (Dropout)         (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 2)                 2050      \n",
      "=================================================================\n",
      "Total params: 5,159,170\n",
      "Trainable params: 5,159,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 128\n",
    "conv_layers = [[256, 7, 3], [256, 7, 3], [256, 3, -1], [256, 3, -1], [256, 3, -1], [256, 3, 3]]\n",
    "fully_connected_layers = [1024, 1024]\n",
    "num_of_classes = 2\n",
    "threshold = 1e-6\n",
    "dropout_p = 0.5\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model = CharCNNZhang(max_input_size, alphabet_size, embedding_size,\n",
    "                 conv_layers, fully_connected_layers, num_of_classes,\n",
    "                 threshold, dropout_p, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CharCNNZhang model: \n",
      "Train on 1280000 samples, validate on 320000 samples\n",
      "Epoch 1/4\n",
      " - 883s - loss: 0.5014 - acc: 0.7455 - val_loss: 0.4318 - val_acc: 0.8002\n",
      "Epoch 2/4\n",
      " - 875s - loss: 0.4148 - acc: 0.8108 - val_loss: 0.4003 - val_acc: 0.8176\n",
      "Epoch 3/4\n",
      " - 873s - loss: 0.3825 - acc: 0.8290 - val_loss: 0.3881 - val_acc: 0.8257\n",
      "Epoch 4/4\n",
      " - 873s - loss: 0.3585 - acc: 0.8418 - val_loss: 0.3812 - val_acc: 0.8293\n"
     ]
    }
   ],
   "source": [
    "epochs = 4\n",
    "batch_size = 256\n",
    "\n",
    "history = model.train(\n",
    "    training_inputs=X_train,\n",
    "    training_labels=Y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "359/359 [==============================] - 1s 2ms/step\n",
      "score: 0.42604588\n",
      "acc: 0.80222842\n"
     ]
    }
   ],
   "source": [
    "score, acc = model.model.evaluate(X_test, Y_test, batch_size=batch_size)\n",
    "print(\"score: %.8f\" % (score))\n",
    "print(\"acc: %.8f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
