{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "6c53202d-5c34-4859-e7e9-8ef5c7068287"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Reference: https://github.com/nagypeterjob/Sentiment-Analysis-NLTK-ML-LSTM/blob/master/lstm.ipynb\"\"\"\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, CuDNNLSTM, CuDNNGRU, LSTM, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2bc2702e-d6f4-df5f-b80e-50ab23a6d29e"
   },
   "source": [
    "Only keeping the needed columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "89c8c923-c0bf-7b35-9ab8-e63f00b74e5a"
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/sentiment140_train.zip', encoding='ISO-8859-1', header=None, names=['sentiment','id','timestamp','type','user','text'])\n",
    "test_data = pd.read_csv('../data/sentiment140_test.zip', encoding='ISO-8859-1', header=None, names=['sentiment','id','timestamp','type','user','text'])\n",
    "\n",
    "# The original training data are sorted by sentiment value. Shuffle the training data for randomness\n",
    "train_data = train_data[['text','sentiment']].sample(frac=1, random_state=40)\n",
    "test_data = test_data[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize_text(text):\n",
    "    text = text.lower()\n",
    "    # Normalize the empty space symbol\n",
    "    text = re.sub('[\\t\\n]', ' ', text)\n",
    "    # Eliminate non-alphanumeric characters\n",
    "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
    "    # Replace all digits by symbol `DG`\n",
    "    text = re.sub('\\d+', 'DG', text)\n",
    "    return text\n",
    "\n",
    "def preprocess(data):\n",
    "    data['text'] = data['text'].apply(canonicalize_text)\n",
    "\n",
    "preprocess(train_data)\n",
    "preprocess(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text         object\n",
       "sentiment     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>551218</th>\n",
       "      <td>i dont want to leave</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512535</th>\n",
       "      <td>good nite all sleep tight dont let the bed bug...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295295</th>\n",
       "      <td>its a beautiful day guess whos going to see uD...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893979</th>\n",
       "      <td>yay for ikea swedish meatballs</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279338</th>\n",
       "      <td>and god knows how many more singstar and buzz ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886145</th>\n",
       "      <td>rosie_edward thanks you so much for following ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504955</th>\n",
       "      <td>good morning all hope ur day started well</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728643</th>\n",
       "      <td>so bored weekend is almost over</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005200</th>\n",
       "      <td>carlaloo jajanika found our entry woohoo check...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1148569</th>\n",
       "      <td>ooh look who seems to be working tonight way t...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text  sentiment\n",
       "551218                               i dont want to leave           0\n",
       "1512535  good nite all sleep tight dont let the bed bug...          4\n",
       "1295295  its a beautiful day guess whos going to see uD...          4\n",
       "893979                     yay for ikea swedish meatballs           4\n",
       "279338   and god knows how many more singstar and buzz ...          0\n",
       "886145   rosie_edward thanks you so much for following ...          4\n",
       "1504955         good morning all hope ur day started well           4\n",
       "728643                    so bored weekend is almost over           0\n",
       "1005200  carlaloo jajanika found our entry woohoo check...          4\n",
       "1148569  ooh look who seems to be working tonight way t...          4"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_vocabulary(df):\n",
    "    vocabulary = Counter()\n",
    "    for _, row in df.iterrows():\n",
    "        words = row['text'].split()\n",
    "        for word in words:\n",
    "            vocabulary[word] += 1\n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = get_corpus_vocabulary(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the word count histogram of the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 5, 5, 6, 7, 8, 9, 9, 10, 11, 12, 14, 15, 16, 17, 19, 20, 22, 24, 26, 28, 30, 33, 35, 38, 41, 44, 47, 51, 55, 59, 63, 68, 73, 78, 83, 89, 95, 102, 109, 117, 125, 134, 143, 154, 167, 180, 194, 210, 227, 245, 265, 287, 310, 335, 363, 392, 426, 464, 506, 554, 608, 668, 736, 815, 906, 1011, 1133, 1277, 1445, 1643, 1878, 2163, 2508, 2930, 3459, 4132, 5006, 6160, 7727, 9966, 13383, 18923, 28375, 45063, 75112, 129493, 223083, 402773, 611300, 819826]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 100 artists>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAD8CAYAAACLrvgBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAF4dJREFUeJzt3X+sX3Wd5/Hna6iMyKy24N2GaXHLxkaDJCLcQI2TiQsjFDSWPxwWMjs0pGM3EXd0nM1Y5x8yuiaYTIaRrDYh0rFMXLHL6NII2m0qk9n9A+QiLghIuIMgbYDeoQVmJSOD894/vp+uX67fe++npeVb7n0+km++57zP55zP5+SQvjg/vvekqpAkqcevjXsAkqTXD0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVK3ZeMewNH21re+tdasWTPuYUjS68q99977D1U1sVC7RRcaa9asYWpqatzDkKTXlSRP9LTz8pQkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSa9ja7bczpott79m/XWFRpI/SvJgkh8l+XqSNyY5I8ndSaaTfCPJia3tr7f56bZ8zdB2PtPqjyS5eKi+vtWmk2wZqo/sQ5I0HguGRpJVwB8Ck1V1FnACcAXwBeD6qno7cBDY1FbZBBxs9etbO5Kc2dZ7F7Ae+HKSE5KcAHwJuAQ4E7iytWWePiRJY9B7eWoZcFKSZcCbgKeAC4Bb2/LtwGVtekObpy2/MEla/Zaq+nlV/QSYBs5rn+mqeqyqXgJuATa0debqQ5I0BguGRlXtA/4c+CmDsHgeuBd4rqpebs32Aqva9Crgybbuy639qcP1WevMVT91nj4kSWPQc3lqBYOzhDOA3wROZnB56biRZHOSqSRTMzMz4x6OJC1aPZenfgf4SVXNVNU/A98E3gcsb5erAFYD+9r0PuB0gLb8LcCzw/VZ68xVf3aePl6hqm6sqsmqmpyYWPAdIpKkI9QTGj8F1iV5U7vPcCHwEHAn8JHWZiNwW5ve2eZpy79XVdXqV7Snq84A1gLfB+4B1rYnpU5kcLN8Z1tnrj4kSWPQc0/jbgY3o38APNDWuRH4NPCpJNMM7j/c1Fa5CTi11T8FbGnbeRDYwSBwvgtcU1W/aPcsPg7sAh4GdrS2zNOHJGkMMvgf+sVjcnKyfN2rpKXi0A/7Hr/ug69qO0nurarJhdr5i3BJUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1M3QkCR1MzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3RYMjSTvSPLDoc8LST6Z5JQku5M82r5XtPZJckOS6ST3JzlnaFsbW/tHk2wcqp+b5IG2zg3ttbLM1YckaTx6Xvf6SFWdXVVnA+cCLwLfYvAa1z1VtRbY0+YBLmHw/u+1wGZgKwwCALgWOB84D7h2KAS2Ah8dWm99q8/VhyRpDA738tSFwN9X1RPABmB7q28HLmvTG4Cba+AuYHmS04CLgd1VdaCqDgK7gfVt2Zur6q4avHv25lnbGtWHJGkMDjc0rgC+3qZXVtVTbfppYGWbXgU8ObTO3labr753RH2+PiRJY9AdGklOBD4M/PfZy9oZQh3Fcf2K+fpIsjnJVJKpmZmZYzkMSVrSDudM4xLgB1X1TJt/pl1aon3vb/V9wOlD661utfnqq0fU5+vjFarqxqqarKrJiYmJw9glSdLhOJzQuJJfXpoC2AkcegJqI3DbUP2q9hTVOuD5dolpF3BRkhXtBvhFwK627IUk69pTU1fN2taoPiRJY7Csp1GSk4EPAP9xqHwdsCPJJuAJ4PJWvwO4FJhm8KTV1QBVdSDJ54B7WrvPVtWBNv0x4KvAScB32me+PiRJY9AVGlX1M+DUWbVnGTxNNbttAdfMsZ1twLYR9SngrBH1kX1IksbDX4RLkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6dYVGkuVJbk3y4yQPJ3lvklOS7E7yaPte0domyQ1JppPcn+Scoe1sbO0fTbJxqH5ukgfaOje0d4UzVx+StJSt2XI7a7bcPpa+e880vgh8t6reCbwbeBjYAuypqrXAnjYPcAmwtn02A1thEADAtcD5wHnAtUMhsBX46NB661t9rj4kSWOwYGgkeQvw28BNAFX1UlU9B2wAtrdm24HL2vQG4OYauAtYnuQ04GJgd1UdqKqDwG5gfVv25qq6q71f/OZZ2xrVhyRpDHrONM4AZoC/SnJfkq8kORlYWVVPtTZPAyvb9CrgyaH197bafPW9I+rM08crJNmcZCrJ1MzMTMcuSZKORE9oLAPOAbZW1XuAnzHrMlE7Q6ijP7y+PqrqxqqarKrJiYmJYzkMSVrSekJjL7C3qu5u87cyCJFn2qUl2vf+tnwfcPrQ+qtbbb766hF15ulDkjQGC4ZGVT0NPJnkHa10IfAQsBM49ATURuC2Nr0TuKo9RbUOeL5dYtoFXJRkRbsBfhGwqy17Icm69tTUVbO2NaoPSdIYLOts95+AryU5EXgMuJpB4OxIsgl4Ari8tb0DuBSYBl5sbamqA0k+B9zT2n22qg606Y8BXwVOAr7TPgDXzdGHJGkMukKjqn4ITI5YdOGItgVcM8d2tgHbRtSngLNG1J8d1YckaTz8RbgkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkbl2hkeTxJA8k+WGSqVY7JcnuJI+27xWtniQ3JJlOcn+Sc4a2s7G1fzTJxqH6uW37023dzNeHJGk8DudM499V1dlVdegNfluAPVW1FtjT5gEuAda2z2ZgKwwCALgWOB84D7h2KAS2Ah8dWm/9An1Iksbg1Vye2gBsb9PbgcuG6jfXwF3A8iSnARcDu6vqQFUdBHYD69uyN1fVXe1VsTfP2taoPiRJY9AbGgX8zyT3Jtncaiur6qk2/TSwsk2vAp4cWndvq81X3zuiPl8fkqQxWNbZ7reqal+Sfw3sTvLj4YVVVUnq6A+vr48WZJsB3va2tx3LYUjSktZ1plFV+9r3fuBbDO5JPNMuLdG+97fm+4DTh1Zf3Wrz1VePqDNPH7PHd2NVTVbV5MTERM8uSZKOwIKhkeTkJP/q0DRwEfAjYCdw6AmojcBtbXoncFV7imod8Hy7xLQLuCjJinYD/CJgV1v2QpJ17ampq2Zta1QfkqQx6Lk8tRL4VnsKdhnw36rqu0nuAXYk2QQ8AVze2t8BXApMAy8CVwNU1YEknwPuae0+W1UH2vTHgK8CJwHfaR+A6+boQ5I0BguGRlU9Brx7RP1Z4MIR9QKumWNb24BtI+pTwFm9fUiSxsNfhEuSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnq1h0aSU5Icl+Sb7f5M5LcnWQ6yTeSnNjqv97mp9vyNUPb+EyrP5Lk4qH6+labTrJlqD6yD0nSeBzOmcYngIeH5r8AXF9VbwcOAptafRNwsNWvb+1IciZwBfAuYD3w5RZEJwBfAi4BzgSubG3n60OSNAZdoZFkNfBB4CttPsAFwK2tyXbgsja9oc3Tll/Y2m8Abqmqn1fVTxi8Q/y89pmuqseq6iXgFmDDAn1Iksag90zjL4E/Af6lzZ8KPFdVL7f5vcCqNr0KeBKgLX++tf//9VnrzFWfrw9JWlLWbLmdNVtuH/cwFg6NJB8C9lfVva/BeI5Iks1JppJMzczMjHs4krRo9ZxpvA/4cJLHGVw6ugD4IrA8ybLWZjWwr03vA04HaMvfAjw7XJ+1zlz1Z+fp4xWq6saqmqyqyYmJiY5dkiQdiQVDo6o+U1Wrq2oNgxvZ36uq3wPuBD7Smm0EbmvTO9s8bfn3qqpa/Yr2dNUZwFrg+8A9wNr2pNSJrY+dbZ25+pAkjcGr+Z3Gp4FPJZlmcP/hpla/CTi11T8FbAGoqgeBHcBDwHeBa6rqF+2exceBXQyeztrR2s7XhyRpDJYt3OSXqupvgb9t048xePJpdpt/An53jvU/D3x+RP0O4I4R9ZF9SJLGw1+ES5K6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhSeq2YGgkeWOS7yf5P0keTPJnrX5GkruTTCf5RntVK+11rt9o9buTrBna1mda/ZEkFw/V17fadJItQ/WRfUiSxqPnTOPnwAVV9W7gbGB9knXAF4Drq+rtwEFgU2u/CTjY6te3diQ5k8H7v98FrAe+nOSEJCcAXwIuAc4ErmxtmacPSdIYLBgaNfB/2+wb2qeAC4BbW307cFmb3tDmacsvTJJWv6Wqfl5VPwGmGbzK9Txguqoeq6qXgFuADW2dufqQJI1B1z2NdkbwQ2A/sBv4e+C5qnq5NdkLrGrTq4AnAdry54FTh+uz1pmrfuo8fUiSxqArNKrqF1V1NrCawZnBO4/pqA5Tks1JppJMzczMjHs4krRoHdbTU1X1HHAn8F5geZJlbdFqYF+b3gecDtCWvwV4drg+a5256s/O08fscd1YVZNVNTkxMXE4uyRJOgw9T09NJFnepk8CPgA8zCA8PtKabQRua9M72zxt+feqqlr9ivZ01RnAWuD7wD3A2vak1IkMbpbvbOvM1YckaQyWLdyE04Dt7SmnXwN2VNW3kzwE3JLkvwD3ATe19jcBf51kGjjAIASoqgeT7AAeAl4GrqmqXwAk+TiwCzgB2FZVD7ZtfXqOPiRJY7BgaFTV/cB7RtQfY3B/Y3b9n4DfnWNbnwc+P6J+B3BHbx+SpPHwF+GSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJ6mZoSJK6GRqSpG6GhiSpm6EhScepNVtuZ82W28c9jFcwNCRJ3QwNSVK3nneEn57kziQPJXkwySda/ZQku5M82r5XtHqS3JBkOsn9Sc4Z2tbG1v7RJBuH6ucmeaCtc0OSzNeHJGk8es40Xgb+uKrOBNYB1yQ5E9gC7KmqtcCeNg9wCbC2fTYDW2EQAMC1wPkMXuF67VAIbAU+OrTe+lafqw9J0hgsGBpV9VRV/aBN/yPwMLAK2ABsb822A5e16Q3AzTVwF7A8yWnAxcDuqjpQVQeB3cD6tuzNVXVXVRVw86xtjepDkjQGh3VPI8ka4D3A3cDKqnqqLXoaWNmmVwFPDq22t9Xmq+8dUWeePmaPa3OSqSRTMzMzh7NLkqTD0B0aSX4D+Bvgk1X1wvCydoZQR3lsrzBfH1V1Y1VNVtXkxMTEsRyGJC1pXaGR5A0MAuNrVfXNVn6mXVqife9v9X3A6UOrr261+eqrR9Tn60OSNAY9T08FuAl4uKr+YmjRTuDQE1AbgduG6le1p6jWAc+3S0y7gIuSrGg3wC8CdrVlLyRZ1/q6ata2RvUhSRqDZR1t3gf8PvBAkh+22p8C1wE7kmwCngAub8vuAC4FpoEXgasBqupAks8B97R2n62qA236Y8BXgZOA77QP8/QhSRqDBUOjqv43kDkWXziifQHXzLGtbcC2EfUp4KwR9WdH9SFJGg9/ES5J6mZoSJK6GRqSpG6GhiSpm6EhSepmaEiSuhkakqRuhoYkqZuhIUnqZmhIkroZGpKkboaGJKmboSFJx5E1W25nzZbbxz2MORkakqRuhoYkqZuhIUnq1vO6121J9if50VDtlCS7kzzavle0epLckGQ6yf1JzhlaZ2Nr/2iSjUP1c5M80Na5ob3ydc4+JEnj03Om8VVg/azaFmBPVa0F9rR5gEuAte2zGdgKgwAArgXOB84Drh0Kga3AR4fWW79AH5KkMVkwNKrq74ADs8obgO1tejtw2VD95hq4C1ie5DTgYmB3VR2oqoPAbmB9W/bmqrqrvSb25lnbGtWHJGlMjvSexsqqeqpNPw2sbNOrgCeH2u1ttfnqe0fU5+tDkjQmr/pGeDtDqKMwliPuI8nmJFNJpmZmZo7lUCRpSTvS0HimXVqife9v9X3A6UPtVrfafPXVI+rz9fErqurGqpqsqsmJiYkj3CVJ0kKONDR2AoeegNoI3DZUv6o9RbUOeL5dYtoFXJRkRbsBfhGwqy17Icm69tTUVbO2NaoPSdKYLFuoQZKvA+8H3ppkL4OnoK4DdiTZBDwBXN6a3wFcCkwDLwJXA1TVgSSfA+5p7T5bVYdurn+MwRNaJwHfaR/m6UOSNCYLhkZVXTnHogtHtC3gmjm2sw3YNqI+BZw1ov7sqD4kSePjL8IlSd0MDUkas+P9L9sOMzQkSd0MDUlSN0NDktTN0JAkdTM0JEndDA1JUjdDQ5LG4PX0mO0wQ0OS1M3QkCR1MzQkSd0MDUlSN0NDkl4jr9eb38MMDUlSN0NDko6hxXB2Mey4D40k65M8kmQ6yZZxj0eSFrLYgmLYgm/uG6ckJwBfAj4A7AXuSbKzqh4a78gk6ZeGA+Lx6z44xpEce8d1aADnAdNV9RhAkluADYChIek1t1jPHg7H8R4aq4Anh+b3AuePaSySjhOH/vF+/LoPHta0Xr1U1bjHMKckHwHWV9UftPnfB86vqo/ParcZ2Nxm3wE8cphdvRX4h1c53Ncb93lpcJ+XhqOxz/+mqiYWanS8n2nsA04fml/daq9QVTcCNx5pJ0mmqmrySNd/PXKflwb3eWl4Lff5eH966h5gbZIzkpwIXAHsHPOYJGnJOq7PNKrq5SQfB3YBJwDbqurBMQ9Lkpas4zo0AKrqDuCOY9zNEV/aeh1zn5cG93lpeM32+bi+ES5JOr4c7/c0JEnHkSUfGkvhz5QkOT3JnUkeSvJgkk+0+ilJdid5tH2vGPdYj6YkJyS5L8m32/wZSe5ux/ob7eGKRSXJ8iS3JvlxkoeTvHcJHOc/av9d/yjJ15O8cbEd6yTbkuxP8qOh2sjjmoEb2r7fn+ScozmWJR0aQ3+m5BLgTODKJGeOd1THxMvAH1fVmcA64Jq2n1uAPVW1FtjT5heTTwAPD81/Abi+qt4OHAQ2jWVUx9YXge9W1TuBdzPY/0V7nJOsAv4QmKyqsxg8MHMFi+9YfxVYP6s213G9BFjbPpuBrUdzIEs6NBj6MyVV9RJw6M+ULCpV9VRV/aBN/yODf0hWMdjX7a3ZduCy8Yzw6EuyGvgg8JU2H+AC4NbWZFHtL0CStwC/DdwEUFUvVdVzLOLj3CwDTkqyDHgT8BSL7FhX1d8BB2aV5zquG4Cba+AuYHmS047WWJZ6aIz6MyWrxjSW10SSNcB7gLuBlVX1VFv0NLByTMM6Fv4S+BPgX9r8qcBzVfVym1+Mx/oMYAb4q3ZZ7itJTmYRH+eq2gf8OfBTBmHxPHAvi/9Yw9zH9Zj+u7bUQ2NJSfIbwN8An6yqF4aX1eAxukXxKF2SDwH7q+recY/lNbYMOAfYWlXvAX7GrEtRi+k4A7Tr+BsYBOZvAifzq5dxFr3X8rgu9dDo+jMli0GSNzAIjK9V1Tdb+ZlDp63te/+4xneUvQ/4cJLHGVxyvIDBtf7l7RIGLM5jvRfYW1V3t/lbGYTIYj3OAL8D/KSqZqrqn4FvMjj+i/1Yw9zH9Zj+u7bUQ2NJ/JmSdj3/JuDhqvqLoUU7gY1teiNw22s9tmOhqj5TVaurag2DY/q9qvo94E7gI63ZotnfQ6rqaeDJJO9opQsZvEZgUR7n5qfAuiRvav+dH9rnRX2sm7mO607gqvYU1Trg+aHLWK/akv9xX5JLGVz/PvRnSj4/5iEddUl+C/hfwAP88hr/nzK4r7EDeBvwBHB5Vc2+2fa6luT9wH+uqg8l+bcMzjxOAe4D/kNV/Xyc4zvakpzN4Ob/icBjwNUM/udw0R7nJH8G/HsGTwneB/wBg2v4i+ZYJ/k68H4Gf832GeBa4H8w4ri28PyvDC7TvQhcXVVTR20sSz00JEn9lvrlKUnSYTA0JEndDA1JUjdDQ5LUzdCQJHUzNCRJ3QwNSVI3Q0OS1O3/AeV9Xs3V9dA9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_total_cnt = sum(vocabulary.values())\n",
    "sorted_by_wd_cnt = sorted(((cnt, wd) for wd, cnt in vocabulary.items()), reverse=True)\n",
    "\n",
    "plt_points = []\n",
    "accu = 0\n",
    "j = 0\n",
    "for i in range(1, 101):\n",
    "    while accu < (corpus_total_cnt * i / 100.0):\n",
    "        accu += sorted_by_wd_cnt[j][0]\n",
    "        j += 1\n",
    "    plt_points.append(j)\n",
    "    \n",
    "print(plt_points)\n",
    "plt.bar(range(1, 101), plt_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 10000 words account for 90% word occurance of the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the training corpus: 819826\n",
      "The top 10 words are: [(750807, 'i'), (564547, 'to'), (520035, 'the'), (377569, 'a'), (314054, 'my'), (298347, 'and'), (270065, 'you'), (236007, 'is'), (230839, 'it'), (215690, 'for')]\n",
      "The least 10 words are: [(1, 'DG_DGmagazine'), (1, 'DG_DGchick'), (1, 'DG_DG_manytweets'), (1, 'DG_DG_hustle_ent'), (1, 'DG_'), (1, 'DG^o'), (1, 'DG^eDG'), (1, 'DG^c'), (1, 'DG^'), (1, 'DG\\\\quot')]\n"
     ]
    }
   ],
   "source": [
    "print('The size of the training corpus: {}'.format(len(vocabulary)))\n",
    "print('The top 10 words are: {}'.format(sorted_by_wd_cnt[:10]))\n",
    "print('The least 10 words are: {}'.format(sorted_by_wd_cnt[-10:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>stellargirl i loooooooovvvvvveee my kindleDG n...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>reading my kindleDG  love it lee childs is goo...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ok first assesment of the kindleDG it fucking ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kenburbary youll love your kindleDG ive had mi...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mikefish  fair enough but i have the kindleDG ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>richardebaker no it is too big im quite happy ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fuck this economy i hate aig and their non loa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>jquery is my new best friend</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>loves twitter</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>how can you not love obama he makes jokes abou...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  sentiment\n",
       "0  stellargirl i loooooooovvvvvveee my kindleDG n...          4\n",
       "1  reading my kindleDG  love it lee childs is goo...          4\n",
       "2  ok first assesment of the kindleDG it fucking ...          4\n",
       "3  kenburbary youll love your kindleDG ive had mi...          4\n",
       "4  mikefish  fair enough but i have the kindleDG ...          4\n",
       "5  richardebaker no it is too big im quite happy ...          4\n",
       "6  fuck this economy i hate aig and their non loa...          0\n",
       "7                       jquery is my new best friend          4\n",
       "8                                      loves twitter          4\n",
       "9  how can you not love obama he makes jokes abou...          4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 2])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows with neural sentiment score from the test data, since they don't exist in the train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = test_data[test_data['sentiment'].isin([0, 4])]\n",
    "test_data['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rows: 800000\n",
      "Negative rows: 800000\n"
     ]
    }
   ],
   "source": [
    "print('Positive rows: {}'.format(train_data[ train_data['sentiment'] == 4]['sentiment'].size))\n",
    "print('Negative rows: {}'.format(train_data[ train_data['sentiment'] == 0]['sentiment'].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "43632d2d-6160-12ce-48b0-e5eb1c207076"
   },
   "outputs": [],
   "source": [
    "# Only keep top 10000 words in the corpus.\n",
    "max_fatures = 10000\n",
    "tokenizer = Tokenizer(num_words=max_fatures, split=' ')\n",
    "tokenizer.fit_on_texts(train_data['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenizer.texts_to_sequences(train_data['text'].values)\n",
    "X_train = pad_sequences(X_train)\n",
    "\n",
    "X_test = tokenizer.texts_to_sequences(test_data['text'].values)\n",
    "X_test = pad_sequences(X_test, maxlen=X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train: (1600000, 40)\n",
      "The shape of X_test: (359, 40)\n"
     ]
    }
   ],
   "source": [
    "print('The shape of X_train: {}'.format(X_train.shape))\n",
    "print('The shape of X_test: {}'.format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   1\n",
      "  40  70   2 343]\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    1    5   26   19    3\n",
      " 9685    8  196   20    3   12    8 1010   11   25  485  112]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[0])\n",
    "print(X_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "b35748b8-2353-3db2-e571-5fd22bb93eb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of Y_train: (1600000, 2)\n",
      "The shape of Y_test: (359, 2)\n"
     ]
    }
   ],
   "source": [
    "Y_train = pd.get_dummies(train_data['sentiment']).values\n",
    "Y_test = pd.get_dummies(test_data['sentiment']).values\n",
    "print('The shape of Y_train: {}'.format(Y_train.shape))\n",
    "print('The shape of Y_test: {}'.format(Y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]]\n",
      "[[0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [0 1]\n",
      " [0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(Y_train[:10])\n",
    "print(Y_test[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_limit = len(X_train)\n",
    "X_train_sample, Y_train_sample = X_train[:size_limit], Y_train[:size_limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should explore LSTM, CuDNNLSTM, GRU, CuDNNGRU cells. CuDNN cells have following limitations: 1. The activation function has to be `tanh`. 2. It doesn't support dropout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cudnn_lstm_model(input_length, max_fatures, embed_dim, lstm_out, dropout=0):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_fatures, embed_dim, input_length=input_length))\n",
    "    \n",
    "    # Given shape(x) = [samples, timesteps, channels], it uses noise_shape = [samples, 1, channels] and drops \n",
    "    # entire 1-D feature maps. Dropout matrix with this shape applies the same dropout rate at all timesteps.\n",
    "    model.add(SpatialDropout1D(rate=dropout))\n",
    "    # input dropout and recurrent dropout are not available for CuDNN celss\n",
    "    model.add(CuDNNLSTM(lstm_out))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def get_cudnn_gru_model(input_length, max_fatures, embed_dim, lstm_out, dropout=0):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_fatures, embed_dim, input_length=input_length))\n",
    "    \n",
    "    # Given shape(x) = [samples, timesteps, channels], it uses noise_shape = [samples, 1, channels] and drops \n",
    "    # entire 1-D feature maps. Dropout matrix with this shape applies the same dropout rate at all timesteps.\n",
    "    model.add(SpatialDropout1D(rate=dropout))\n",
    "    # input dropout and recurrent dropout are not available for CuDNN celss\n",
    "    model.add(CuDNNGRU(lstm_out, return_sequences=True))\n",
    "    model.add(CuDNNGRU(lstm_out))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def get_cpu_model(input_length, max_fatures, embed_dim, lstm_out, input_dropout=0, recurrent_dropout=0):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_fatures, embed_dim, input_length=input_length))\n",
    "\n",
    "    model.add(LSTM(lstm_out, dropout=input_dropout, recurrent_dropout=recurrent_dropout))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters tuning for LSTM: https://arxiv.org/pdf/1707.06799.pdf\n",
    "embed_dim = 200\n",
    "lstm_out = 100\n",
    "# Why 0.5: https://www.reddit.com/r/MachineLearning/comments/3oztvk/why_50_when_using_dropout/\n",
    "# Variational Dropout: https://arxiv.org/pdf/1512.05287.pdf\n",
    "input_dropout = 0.5\n",
    "recurrent_dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 40, 200)           2000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_2 (CuDNNLSTM)     (None, 100)               120800    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 202       \n",
      "=================================================================\n",
      "Total params: 2,121,002\n",
      "Trainable params: 2,121,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/7\n",
      " - 150s - loss: 0.4227 - acc: 0.8045\n",
      "Epoch 2/7\n",
      " - 115s - loss: 0.3906 - acc: 0.8227\n",
      "Epoch 3/7\n",
      " - 115s - loss: 0.3769 - acc: 0.8302\n",
      "Epoch 4/7\n",
      " - 115s - loss: 0.3667 - acc: 0.8356\n",
      "Epoch 5/7\n",
      " - 115s - loss: 0.3587 - acc: 0.8396\n",
      "Epoch 6/7\n",
      " - 115s - loss: 0.3526 - acc: 0.8431\n",
      "Epoch 7/7\n",
      " - 115s - loss: 0.3476 - acc: 0.8454\n",
      "score: 0.36588734\n",
      "acc: 0.82729805\n"
     ]
    }
   ],
   "source": [
    "cudnn_lstm_model = get_cudnn_lstm_model(X_train.shape[1], max_fatures, embed_dim, lstm_out, input_dropout)\n",
    "\n",
    "batch_size = 256\n",
    "cudnn_lstm_model.fit(X_train_sample, Y_train_sample, epochs = 7, batch_size=batch_size, verbose = 2)\n",
    "\n",
    "score, acc = cudnn_lstm_model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.8f\" % (score))\n",
    "print(\"acc: %.8f\" % (acc))\n",
    "\n",
    "cudnn_lstm_model.save('../models/cudnn_lstm_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "_cell_guid": "d5e499ac-2eba-6ff7-8d9a-ff65eb04099b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      " - 431s - loss: 0.3807 - acc: 0.8286\n",
      "Epoch 2/7\n",
      " - 432s - loss: 0.3647 - acc: 0.8366\n",
      "Epoch 3/7\n",
      " - 432s - loss: 0.3522 - acc: 0.8432\n",
      "Epoch 4/7\n",
      " - 432s - loss: 0.3421 - acc: 0.8482\n",
      "Epoch 5/7\n",
      " - 432s - loss: 0.3335 - acc: 0.8525\n",
      "Epoch 6/7\n",
      " - 432s - loss: 0.3264 - acc: 0.8563\n",
      "Epoch 7/7\n",
      " - 431s - loss: 0.3207 - acc: 0.8592\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff379ab3630>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "cudnn_lstm_model.fit(X_train_sample, Y_train_sample, epochs = 7, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4ebd7bc1-53c0-0e31-a0b0-b6d0a3017434"
   },
   "source": [
    "Extracting a validation set, and measuring score and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "_cell_guid": "a970f412-722f-6d6d-72c8-325d0901ccef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.37152850\n",
      "acc: 0.83286907\n"
     ]
    }
   ],
   "source": [
    "score, acc = cudnn_lstm_model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.8f\" % (score))\n",
    "print(\"acc: %.8f\" % (acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 40, 200)           2000000   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_8 (Spatial (None, 40, 200)           0         \n",
      "_________________________________________________________________\n",
      "cu_dnngru_1 (CuDNNGRU)       (None, 40, 500)           1053000   \n",
      "_________________________________________________________________\n",
      "cu_dnngru_2 (CuDNNGRU)       (None, 500)               1503000   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 1002      \n",
      "=================================================================\n",
      "Total params: 4,557,002\n",
      "Trainable params: 4,557,002\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "cudnn_gru_model = get_cudnn_gru_model(X_train.shape[1], max_fatures, embed_dim, lstm_out, input_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      " - 773s - loss: 0.4213 - acc: 0.8053\n",
      "Epoch 2/7\n",
      " - 770s - loss: 0.3868 - acc: 0.8248\n",
      "Epoch 3/7\n",
      " - 771s - loss: 0.3714 - acc: 0.8336\n",
      "Epoch 4/7\n",
      " - 771s - loss: 0.3599 - acc: 0.8395\n",
      "Epoch 5/7\n",
      " - 771s - loss: 0.3517 - acc: 0.8435\n",
      "Epoch 6/7\n",
      " - 771s - loss: 0.3466 - acc: 0.8465\n",
      "Epoch 7/7\n",
      " - 772s - loss: 0.3433 - acc: 0.8482\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff37cc0d630>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "cudnn_gru_model.fit(X_train_sample, Y_train_sample, epochs = 7, batch_size=batch_size, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score: 0.39665206\n",
      "acc: 0.81615599\n"
     ]
    }
   ],
   "source": [
    "score, acc = cudnn_gru_model.evaluate(X_test, Y_test, verbose = 2, batch_size = batch_size)\n",
    "print(\"score: %.8f\" % (score))\n",
    "print(\"acc: %.8f\" % (acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve:\n",
    "1. The model is suffering from overfitting. We should add more dropout layers and try other regularization methods.\n",
    "2. Systematically search for the optimal combination of hyperparameters\n",
    "3. Try pretrained embedding, bidirectional RNN, combination of word and character level embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 185,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
