{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import importlib\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../data/sentiment140_train.zip', encoding='ISO-8859-1', header=None, names=['sentiment','id','timestamp','type','user','text'])\n",
    "test_data = pd.read_csv('../data/sentiment140_test.zip', encoding='ISO-8859-1', header=None, names=['sentiment','id','timestamp','type','user','text'])\n",
    "\n",
    "train_data = train_data[['text','sentiment']]\n",
    "test_data = test_data[['text','sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:\n",
      "text         object\n",
      "sentiment     int64\n",
      "dtype: object\n",
      "test_data:\n",
      "text         object\n",
      "sentiment     int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print('train_data:')\n",
    "print(train_data.dtypes)\n",
    "print('test_data:')\n",
    "print(test_data.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive rows: 800000\n",
      "Negative rows: 800000\n"
     ]
    }
   ],
   "source": [
    "print('Positive rows: {}'.format(train_data[ train_data['sentiment'] == 4]['sentiment'].size))\n",
    "print('Negative rows: {}'.format(train_data[ train_data['sentiment'] == 0]['sentiment'].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = shuffle(train_data, random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 I don't want to leave \n",
      "4 good nite all! sleep tight don't let the bed bugs bite \n",
      "4 It's a beautiful day! Guess who's going to see U2 in concert this fall?  yep - me! B/c @jaredbridges is the most amazing husband ever!!!!\n",
      "4 yay for ikea swedish meatballs \n",
      "0 and God knows how many more SingStar and Buzz iterations announced. Rock Band Unplugged to be awesome, OnLive to be absent.  #e3\n",
      "4 @rosie_edward Thanks you so much for following me on Twitter; I hope you find it as exciting as I do! Looking forward to your tweets! \n",
      "4 good morning all... hope ur day started well \n",
      "0 so bored.. weekend is almost over \n",
      "4 @carlaloo @jajanika Found our entry! Woohoo! Check them out. I entered 2 pictures.  I hope we win. *crossed fingers*\n",
      "4 Ooh look who seems to be working tonight... way to go twitter! \n",
      "0 getting back to office on a Monday after a totally unscheduled off is so .... err.... Sad \n",
      "0 is in Concord, sick at M&amp;D's house while everyone else is at church. \n",
      "0 makingg youtubes with myy cousin; bahaha.. grrrrr, he's goingg to vegas \n",
      "4 to whom it may concern: Whores ain't suppose to eat salad this damn early in the morning lmmfao \n",
      "0 So how's everyone enjoying their new iPhones? I'm a tad jealous, but that service plan is still stopping me \n",
      "4 @fuckcity Happy birthday love.&lt;3  Hope it's a good one.\n",
      "0 @brittneygreen you were sleepy though! \n",
      "0 mornnnninggg. ugh by cub has gone to work without a phoneee. got no one to textt \n",
      "4 Why smileys are like this =&gt;  and not like this =&gt; (-: ???\n",
      "0 homeworkkkk \n",
      "4 @besserwerber Get 100 followers a day using www.tweeteradder.com Once you add everyone you are on the train or pay vip \n",
      "0 Im trying 2 recover,too much chocs r bad if dats possible.... \n",
      "4 everyone should follow me         AND feed me mexican everyday!\n",
      "4 Back to watching the top 1000 song on Max - nearning #1 Have a great day to all of you \n",
      "4 @JessiDavis_ haha u never know he could come around! \n",
      "4 ready to lay out! \n",
      "4 @tpatt well actually maybe soon but more like open source stuff for education. you've done so much in your life you could write a novel \n",
      "0 @theworldfamous no I have been out of town and my phone won't do it...I need to get to a computer \n",
      "4 @robsaker - well you know when I switch over to Vista using Bootcamp I get it \n",
      "4 @missohlaura sure it will  my tumblr is like my life on screen.\n",
      "4 @AndreaDoria1021 It's sooooo good. \n",
      "0 @Helinacoustic I knows \n",
      "4 @CarinBerger Yay! Glad I could help. \n",
      "4 @kristenstewart9 cause you were surrounded by fans that love you so much  and i kinda thought you shoulda been used to it lol but you're\n",
      "4 is sooo tired Booo \n",
      "0 @EsmaaSelf  You live in an area of the country that I love to death. My wife and I try to get there every year, not this year though \n",
      "4 IS LOVING HIS NEW NOKIA E71 \n",
      "0 @Hannah_oxberry oh dear  doesn't sound good....\n",
      "0 WTF? as if salopek's omitted \n",
      "4 shiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii \n",
      "4 k so my favorite movie ever is on. Home Alone 2  abc\n",
      "0 Bad day \n",
      "4 @foodshethought lol, i think i have been doing this for 3 years \n",
      "4 @J9Naimoli The horror that is Twilight is spreading. And no, we won't like it \n",
      "0 @tdmalone I was laid off from my drafting job due to the recession... again \n",
      "4 having a movie night with some drinks  Going to watch Casino Royale, then The strangers\n",
      "4 The Hills is on!!!!!! \n",
      "4 @scottrmcgrew I know sleep is so over rated! who needs it, right! \n",
      "4 at pride parade all day with Bina, Kristi, Heather,and C.      - June 6\n",
      "0 ...oh! To bad I dont have it \n",
      "0 just home from being with mates, won no money gambling \n"
     ]
    }
   ],
   "source": [
    "def print_df_head(df, limit=50):\n",
    "    for i, row in enumerate(df.iterrows()):\n",
    "        print(row[1]['sentiment'], row[1]['text'])\n",
    "        if i == limit:\n",
    "            break\n",
    "            \n",
    "print_df_head(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove twitter @ tags\n",
    "def remove_twitter_at(text):\n",
    "    # Get rid of the leading @\n",
    "    text = re.sub('^@.*? ', '', text)\n",
    "    \n",
    "    # Get rid of @ in the middle of the tweet\n",
    "    text = re.sub(' @.*? ', ' ', text)\n",
    "    \n",
    "    # Get rid of tailing @\n",
    "    text = re.sub(' @.*?$', '', text)\n",
    "    \n",
    "    return text\n",
    "    \n",
    "train_data['text'] = train_data['text'].apply(remove_twitter_at)\n",
    "test_data['text'] = test_data['text'].apply(remove_twitter_at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table to map punctuations to None\n",
    "tbl = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "        if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    return text.translate(tbl)\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(remove_punctuations)\n",
    "test_data['text'] = test_data['text'].apply(remove_punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only remove separate digits, not digits within word, such as `U2`\n",
    "def remove_digits(text):\n",
    "    # Remove leading digits\n",
    "    text = re.sub('^\\d+? ', ' ', text)\n",
    "    \n",
    "    # Remove other digits\n",
    "    text = re.sub(' \\d+', ' ', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(remove_digits)\n",
    "test_data['text'] = test_data['text'].apply(remove_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(to_lowercase)\n",
    "test_data['text'] = test_data['text'].apply(to_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split a string to a list of words\n",
    "def split(text):\n",
    "    return text.split()\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(split)\n",
    "test_data['text'] = test_data['text'].apply(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.quora.com/Is-there-a-stop-word-list-specifically-designed-for-sentiment-analysis\n",
    "# As stated before, the general nltk stopwords may have a negative impacts on sentiment analysis, as it removes\n",
    "# negation words such as don't. Here I build a minimal set of stopwords specific to sentiment analysis. \n",
    "\n",
    "stopwords = [\n",
    "    'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \n",
    "    \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \n",
    "    'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', \n",
    "    'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", \n",
    "    'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', \n",
    "    'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and'\n",
    "]\n",
    "\n",
    "def remove_stopwords(arr):\n",
    "    arr = [word for word in arr if word not in stopwords]\n",
    "    return arr\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(remove_stopwords)\n",
    "test_data['text'] = test_data['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Transform word from the derived form to root form when applicable \n",
    "def stem_words(arr):\n",
    "    arr = [stemmer.stem(word) for word in arr]\n",
    "    return arr\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(stem_words)\n",
    "test_data['text'] = test_data['text'].apply(stem_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_corpus_vocabulary(df):\n",
    "    vocabulary = Counter()\n",
    "    for _, row in df.iterrows():\n",
    "        words = row['text']\n",
    "        for word in words:\n",
    "            vocabulary[word] += 1\n",
    "    return vocabulary\n",
    "\n",
    "vocabulary = get_corpus_vocabulary(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the training corpus: 465158\n",
      "The top 10 words are: [(563975, 'to'), (215064, 'for'), (214112, 'in'), (183254, 'of'), (176921, 'im'), (166902, 'on'), (150638, 'so'), (136967, 'go'), (127416, 'but'), (124973, 'just')]\n",
      "The least 10 words are: [(1, '$$$down'), (1, '$$$dinerocashbucksgreenbenjaminschangeoth'), (1, '$$$1500'), (1, '$$$+'), (1, '$$$$$they'), (1, '$$$$$$fgdf^amp^^^^'), (1, '$$$$$$$$$$$$$'), (1, '$$$$$$$$$$$$'), (1, '$$$$$$$$$$$'), (1, '$$$$$$$$$')]\n",
      "[1, 1, 1, 2, 2, 3, 4, 5, 6, 6, 7, 9, 10, 11, 12, 14, 15, 17, 19, 20, 22, 24, 26, 29, 31, 34, 37, 40, 43, 46, 49, 52, 56, 59, 63, 67, 72, 76, 81, 86, 92, 97, 103, 110, 117, 124, 132, 141, 150, 160, 171, 182, 193, 206, 219, 233, 248, 264, 280, 298, 317, 338, 359, 383, 409, 436, 466, 499, 536, 577, 621, 671, 726, 787, 853, 927, 1011, 1105, 1213, 1334, 1472, 1633, 1817, 2033, 2292, 2602, 2978, 3443, 4030, 4788, 5807, 7215, 9346, 12838, 19175, 32595, 66621, 161824, 313491, 465158]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def show_vocabulary_summary(vocabulary):\n",
    "    corpus_total_cnt = sum(vocabulary.values())\n",
    "    sorted_by_wd_cnt = sorted(((cnt, wd) for wd, cnt in vocabulary.items()), reverse=True)\n",
    "\n",
    "    plt_points = []\n",
    "    accu = 0\n",
    "    j = 0\n",
    "    for i in range(1, 101):\n",
    "        while accu < (corpus_total_cnt * i / 100.0):\n",
    "            accu += sorted_by_wd_cnt[j][0]\n",
    "            j += 1\n",
    "        plt_points.append(j)\n",
    "\n",
    "    print('The size of the training corpus: {}'.format(len(vocabulary)))\n",
    "    print('The top 10 words are: {}'.format(sorted_by_wd_cnt[:10]))\n",
    "    print('The least 10 words are: {}'.format(sorted_by_wd_cnt[-10:]))\n",
    "    print(plt_points)\n",
    "    plt.bar(range(1, 101), plt_points)\n",
    "    \n",
    "show_vocabulary_summary(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 20000 words account for 94% word occurance of the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join(arr):\n",
    "    return ' '.join(arr)\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(join)\n",
    "test_data['text'] = test_data['text'].apply(join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 dont want to leav\n",
      "4 good nite all sleep tight dont let bed bug bite\n",
      "4 beauti day guess who go to see u2 in concert fall yep bc most amaz husband ever\n",
      "4 yay for ikea swedish meatbal\n",
      "0 god know how mani more singstar buzz iter announc rock band unplug to awesom onliv to absent e3\n",
      "4 thank so much for follow on twitter hope find as excit as look forward to tweet\n",
      "4 good morn all hope ur day start well\n",
      "0 so bore weekend almost over\n",
      "4 jajanika found entri woohoo check out enter pictur hope win cross finger\n",
      "4 ooh look seem to work tonight way to go twitter\n",
      "0 get back to offic on monday after total unschedul off so err sad\n",
      "0 in concord sick at mampd hous while everyon els at church\n",
      "0 makingg youtub with myy cousin bahaha grrrrr he goingg to vega\n",
      "4 to may concern whore aint suppos to eat salad damn earli in morn lmmfao\n",
      "0 so how everyon enjoy new iphon im tad jealou but servic plan still stop\n",
      "4 happi birthday lovelt3 hope good one\n",
      "0 sleepi though\n",
      "0 mornnnninggg ugh by cub gone to work without phonee got no one to textt\n",
      "4 whi smiley like =gt not like =gt\n",
      "0 homeworkkkk\n",
      "4 get follow day use wwwtweeteraddercom onc add everyon on train or pay vip\n",
      "0 im tri recovertoo much choc r bad if dat possibl\n",
      "4 everyon should follow feed mexican everyday\n",
      "4 back to watch top song on max nearn great day to all of\n",
      "4 haha u never know could come around\n",
      "4 readi to lay out\n",
      "4 well actual mayb soon but more like open sourc stuff for educ youv done so much in life could write novel\n",
      "0 no out of town phone wont iti need to get to comput\n",
      "4 well know when switch over to vista use bootcamp get\n",
      "4 sure will tumblr like life on screen\n",
      "4 sooooo good\n",
      "0 know\n",
      "4 yay glad could help\n",
      "4 caus surround by fan love so much kinda thought shoulda use to lol but your\n",
      "4 sooo tire booo\n",
      "0 live in area of countri love to death wife tri to get there everi year not year though\n",
      "4 love new nokia e71\n",
      "0 oh dear doesnt sound good\n",
      "0 wtf as if salopek omit\n",
      "4 shiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiii\n",
      "4 k so favorit movi ever on home alon abc\n",
      "0 bad day\n",
      "4 lol think for year\n",
      "4 horror twilight spread no wont like\n",
      "0 laid off from draft job due to recess again\n",
      "4 movi night with some drink go to watch casino royal then stranger\n",
      "4 hill on\n",
      "4 know sleep so over rate need right\n",
      "4 at pride parad all day with bina kristi heatherand c june\n",
      "0 oh to bad dont\n",
      "0 just home from with mate won no money gambl\n"
     ]
    }
   ],
   "source": [
    "print_df_head(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 loooooooovvvvvvee kindle2 not dx cool but fantast in own right\n",
      "4 read kindle2 love lee child good read\n",
      "4 ok first asses of kindle2 fuck rock\n",
      "4 youll love kindle2 ive mine for few month never look back new big one huge no need for remors\n",
      "4 fair enough but kindle2 think perfect\n",
      "4 no too big im quit happi with kindle2\n",
      "0 fuck economi hate aig non loan given ass\n",
      "4 jqueri new best friend\n",
      "4 love twitter\n",
      "4 how can not love obama make joke about\n",
      "0 firmli believ obamapelosi zero desir to civil charad slogan but want to destroy conservat\n",
      "4 hous correspond dinner last night whoopi barbara amp sherri went obama got stand ovat\n",
      "4 watchin espnju seen new nike commer with puppet lebronsht hilariouslmao\n",
      "0 dear nike stop with flywir shit wast of scienc ugli love\n",
      "4 lebron best athlet of gener if not all time basketbal relat dont want to get into intersport debat about\n",
      "0 talk to guy last night tell die hard spur fan also told hate lebron jame\n",
      "4 love lebron httpbitlypdhur\n",
      "0 lebron beast but im still cheer atil end\n",
      "4 lebron boss\n",
      "4 lebron hometown hero to lol love laker but let go cav lol\n",
      "4 lebron zydruna such awesom duo\n",
      "4 lebron beast nobodi in nba come even close\n",
      "4 download app for iphon so much fun there liter app for just about anyth\n",
      "4 good news just call from visa offic say everyth finewhat relief sick of scam out there steal\n",
      "4 httptwurlnlepkr4b awesom come back from via\n",
      "4 in montreal for long weekend of rampr much need\n",
      "4 booz allen hamilton bad ass homegrown social collabor platform way cool ttiv\n",
      "4 mluc09 custom innov award winner booz allen hamilton httppingfmc2hpp\n",
      "4 current use nikon d90 love but not as much as canon d50d chose d90 for video featur mistak\n",
      "4 googl alway good place to look shouldv mention work on mustang w dad\n",
      "0 play with android googl phone slide out screen scare would break fucker so fast still prefer iphon\n",
      "0 us plan to resum militari tribun at guantanamo bay onli time on trial will aig exec chrysler debt holder\n",
      "0 omg so bore amp tattoooo so itchi help aha =\n",
      "0 im itchi miser\n",
      "0 no im not itchi for now mayb later lol\n",
      "4 rt love nerdi stanford human biolog video make miss school httpbitly13t7nr\n",
      "4 bit crazi with steep learn curv but lyx realli good for long doc for anyth shorter would insan\n",
      "4 im listen to pyt by danni gokey lt3 lt3 lt3 aww he so amaz lt3 so much\n",
      "4 go to sleep then on bike ride\n",
      "0 cant sleep tooth ach\n",
      "0 blah blah blah same old same old no plan today go back to sleep guess\n",
      "0 glad didnt bay to breaker today freak degre in san francisco wtf\n",
      "0 obama administr must stop bonus to aig ponzi schemer httpbitly2cuig\n",
      "0 start to think citi in realli deep samp^t gonna surviv turmoil or gonna next aig\n",
      "0 shaunwoo haten on aig\n",
      "4 will not regret go to see star trek awesom\n",
      "0 annoy new trend on internet peopl pick apart michael lewi malcolm gladwel nobodi want to read\n",
      "4 highli recommend httptinyurlcomhowdavidbeatsgoliath by malcolm gladwel\n",
      "4 blink by malcolm gladwel amaz book tip point\n",
      "4 malcolm gladwel might new man crush\n",
      "0 omg commerci alon on espn go to drive nut\n"
     ]
    }
   ],
   "source": [
    "print_df_head(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data:\n",
      "[0 4]\n",
      "test_data:\n",
      "[4 0]\n"
     ]
    }
   ],
   "source": [
    "test_data = test_data[test_data['sentiment'].isin([0, 4])]\n",
    "\n",
    "print('train_data:')\n",
    "print(train_data['sentiment'].unique())\n",
    "print('test_data:')\n",
    "print(test_data['sentiment'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600000\n",
      "1596340\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "train_data = train_data[train_data['text'] != '']\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, dev_data = train_test_split(train_data, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_data: 1277072\n",
      "dev_data: 319268\n",
      "test_data: 359\n"
     ]
    }
   ],
   "source": [
    "print('train_data: ' + str(len(train_data)))\n",
    "print('dev_data: ' + str(len(dev_data)))\n",
    "print('test_data: ' + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.to_csv('../data/train_data.csv')\n",
    "dev_data.to_csv('../data/dev_data.csv')\n",
    "test_data.to_csv('../data/test_data.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
